{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing RAG with Contextual Retrieval\n",
    "\n",
    "Retrieval Augmented Generation (RAG) enables Claude to leverage your internal knowledge bases, codebases, or any other corpus of documents when providing a response. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, code generation, and much more.\n",
    "\n",
    "In a [separate guide](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/retrieval_augmented_generation/guide.ipynb), we walked through setting up a basic retrieval system, demonstrated how to evaluate its performance, and then outlined a few techniques to improve performance. In this guide, we present a technique for improving retrieval performance: Contextual Embeddings.\n",
    "\n",
    "In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context. Contextual Embeddings solve this problem by adding relevant context to each chunk before embedding. This method improves the quality of each embedded chunk, allowing for more accurate retrieval and thus better overall performance. Averaged across all data sources we tested, Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35%.\n",
    "\n",
    "The same chunk-specific context can also be used with BM25 search to further improve retrieval performance. We introduce this technique in the “Contextual BM25” section.\n",
    "\n",
    "In this guide, we'll demonstrate how to build and optimize a Contextual Retrieval system using a dataset of 9 codebases as our knowledge base. We'll walk through:\n",
    "\n",
    "1) Setting up a basic retrieval pipeline to establish a baseline for performance.\n",
    "\n",
    "2) Contextual Embeddings: what it is, why it works.\n",
    "\n",
    "3) Implementing Contextual Embeddings and demonstrating performance improvements.\n",
    "\n",
    "4) Contextual BM25: improving performance with *contextual* BM25 hybrid search.\n",
    "\n",
    "5) Improving performance with reranking,\n",
    "\n",
    "### Note:\n",
    "\n",
    "Obtain the [dataset](https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings/data) from the original implementation in anthropic's cookbook.\n",
    "\n",
    "Prompt caching for Bedrock is currently not available - you can refer to [semantic caching](https://aws.amazon.com/blogs/database/improve-speed-and-reduce-cost-for-generative-ai-workloads-with-a-persistent-semantic-cache-in-amazon-memorydb/) for a custom implementation\n",
    "\n",
    "### Evaluation Metrics & Dataset:\n",
    "\n",
    "We use a pre-chunked dataset of 9 codebases - all of which have been chunked according to a basic character splitting mechanism. Our evaluation dataset contains 248 queries - each of which contains a 'golden chunk.' We'll use a metric called Pass@k to evaluate performance. Pass@k checks whether or not the 'golden document' was present in the first k documents retrieved for each query. Contextual Embeddings in this case helped us to improve Pass@10 performance from ~87% --> ~95%.\n",
    "\n",
    "You can find the code files and their chunks in `data/codebase_chunks.json` and the evaluation dataset in `data/evaluation_set.jsonl`\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1) Setup\n",
    "\n",
    "2) Basic RAG\n",
    "\n",
    "3) Contextual Embeddings\n",
    "\n",
    "4) Contextual BM25\n",
    "\n",
    "5) Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need a few libraries, including:\n",
    "\n",
    "1) `boto3` for invoking anthropic claude 3 on bedrock\n",
    "\n",
    "2) `cohere-aws` for invoking cohere rerank model hosted on sagemaker endpoint\n",
    "\n",
    "3) `pandas` and `numpy` for data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 cohere-aws pandas numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch serverless, Sagemaker Jumpstart, Bedrock access\n",
    "\n",
    "Setup a collection for OpenSearch serverless using instructions [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-getting-started.html).  \n",
    "\n",
    "Launch any of the embedding models from [sagemaker jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html#jumpstart-open-use-studio).  \n",
    "**bge-m3** is used here for its long context support up to 8192 input tokens.  \n",
    "You can also use embedding models from bedrock but there's none that support > 2048 input tokens as of this writing.  \n",
    "You can circumvent this limitation by further breaking up the dataset chunks into 2048 tokens or less.  \n",
    "**cohere rerank 3** is used as the reranker.\n",
    "\n",
    "Get access to Bedrock FMs using instructions [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html). **claude-3-haiku** is used as the LLM in this example.\n",
    "\n",
    "[Configure aws credentials](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html) with access to invoke OpenSearch, sagemaker jumpstart and Bedrock endpoints if running this locally.  \n",
    "For Sagemaker Studio notebooks, you can follow the instructions [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) to grant similar access to the sagemaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = '<llm model id>' # bedrock model id\n",
    "embedder_endpoint_name = '<sagemaker endpoint name for embedding model>' # sagemaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class\n",
    "\n",
    "Note that we are using [approximate knn](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) here for search, which is less accurate but faster than the original implementation.  \n",
    "More importantly, the improvements across the different methods used is consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = '<your opensearch region>'\n",
    "service = 'aoss'\n",
    "host_name = '<your opensearch endpoint specific to your collection>'\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key, \n",
    "    credentials.secret_key,\n",
    "    region, \n",
    "    service, \n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "# Build the OpenSearch client\n",
    "open_search_client = OpenSearch(\n",
    "    hosts=[{'host': host_name, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"knn.algo_param.ef_search\": 512,\n",
    "        \"number_of_replicas\": 0,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"engine\": \"faiss\"\n",
    "                }\n",
    "            },\n",
    "            \"doc_id\": {\"type\":\"text\"},\n",
    "            \"original_uuid\": {\"type\":\"text\"},\n",
    "            \"chunk_id\": {\"type\":\"text\"},\n",
    "            \"original_index\": {\"type\":\"integer\"},\n",
    "            \"content\": {\"type\":\"text\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"basic-idx\"\n",
    "\n",
    "# Create index\n",
    "response = open_search_client.indices.create(index_name, body=index_body)\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name: str):\n",
    "        self.sm_runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "    def _query_endpoint(self,encoded_json, content_type):\n",
    "        response = self.sm_runtime_client.invoke_endpoint(EndpointName=embedder_endpoint_name, ContentType=content_type, Body=encoded_json)\n",
    "        return json.loads(response['Body'].read())\n",
    "    def load_data(self, dataset: List[Dict[str, Any]]):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
    "        \n",
    "        with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "            for doc in dataset:\n",
    "                for chunk in doc['chunks']:\n",
    "                    texts_to_embed.append(chunk['content'])\n",
    "                    metadata.append({\n",
    "                        'doc_id': doc['doc_id'],\n",
    "                        'original_uuid': doc['original_uuid'],\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'original_index': chunk['original_index'],\n",
    "                        'content': chunk['content']\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "        \n",
    "        print(f\"Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        with tqdm(total=len(texts), desc=\"Embedding chunks\") as pbar:\n",
    "            result = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i : i + batch_size]\n",
    "                batch_result = self._query_endpoint(json.dumps(batch).encode('utf-8'), 'application/x-text')['embedding']\n",
    "                result.extend(batch_result)\n",
    "                pbar.update(len(batch))\n",
    "        \n",
    "        self.embeddings = result\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.query_endpoint(json.dumps([query]).encode('utf-8'), 'application/x-text')['embedding']\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        query_json = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"embedding\": {\n",
    "                        \"vector\": query_embedding[0], \n",
    "                        \"k\": k}\n",
    "                    },\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = open_search_client.search(\n",
    "            body = query_json,\n",
    "            index = index_name\n",
    "        )\n",
    "\n",
    "        top_hits = response['hits']['hits']\n",
    "\n",
    "        results = list(map(lambda x: {'metadata':{'doc_id':x['_source']['doc_id'],'original_uuid':x['_source']['original_uuid'],'chunk_id':x['_source']['chunk_id'],'original_index':x['_source']['original_index'],'content':x['_source']['content']},'similarity':x['_score']},top_hits))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])\n",
    "\n",
    "    def validate_embedded_chunks(self):\n",
    "        unique_contents = set()\n",
    "        for meta in self.metadata:\n",
    "            unique_contents.add(meta['content'])\n",
    "    \n",
    "        print(f\"Validation results:\")\n",
    "        print(f\"Total embedded chunks: {len(self.metadata)}\")\n",
    "        print(f\"Unique embedded contents: {len(unique_contents)}\")\n",
    "    \n",
    "        if len(self.metadata) != len(unique_contents):\n",
    "            print(\"Warning: There may be duplicate chunks in the embedded data.\")\n",
    "        else:\n",
    "            print(\"All embedded chunks are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load your transformed dataset\n",
    "with open('data/codebase_chunks.json', 'r') as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "base_db = VectorDB(\"base_db\")\n",
    "\n",
    "# Load and process the data\n",
    "base_db.load_data(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk index the documents into AOSS\n",
    "docs = []\n",
    "for i in range(len(base_db.embeddings)):\n",
    "    docs.append({'index':{ \"_index\": index_name}})\n",
    "    docs.append({**base_db.metadata[i],'embedding':base_db.embeddings[i]})\n",
    "    \n",
    "open_search_client.bulk(body=docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Callable, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def evaluate_retrieval(queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20) -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        # Find all golden chunk contents\n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if not golden_doc:\n",
    "                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "            if not golden_chunk:\n",
    "                print(f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieval_function(query, db, k=k)\n",
    "        \n",
    "        # Count how many golden chunks are in the top k retrieved documents\n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['metadata'].get('original_content', doc['metadata'].get('content', '')).strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "    \n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "\n",
    "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents using either VectorDB or ContextualVectorDB.\n",
    "    \n",
    "    :param query: The query string\n",
    "    :param db: The VectorDB or ContextualVectorDB instance\n",
    "    :param k: Number of top results to retrieve\n",
    "    :return: List of retrieved documents\n",
    "    \"\"\"\n",
    "    return db.search(query, k=k)\n",
    "\n",
    "def evaluate_db(db, original_jsonl_path: str, k):\n",
    "    # Load the original JSONL data for queries and ground truth\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Total Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:08<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 76.15%\n",
      "Total Score: 0.7614727342549924\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:08<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 83.54%\n",
      "Total Score: 0.8354454685099846\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:07<00:00, 31.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 88.71%\n",
      "Total Score: 0.8870967741935484\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings\n",
    "\n",
    "With basic RAG, each embedded chunk contains a potentially useful piece of information, but these chunks lack context. With Contextual Embeddings, we create a variation on the embedding itself by adding more context to each text chunk before embedding it. Specifically, we use Claude to create a concise context that explains the chunk using the context of the overall document. In the case of our codebases dataset, we can provide both the chunk and the full file that each chunk was found within to an LLM, then produce the context. Then, we will combine this 'context' and the raw text chunk together into a single text block prior to creating each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"contextual-idx\"\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"knn.algo_param.ef_search\": 512,\n",
    "        \"number_of_replicas\": 0,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"engine\": \"faiss\"\n",
    "                }\n",
    "            },\n",
    "            \"doc_id\": {\"type\":\"text\"},\n",
    "            \"original_uuid\": {\"type\":\"text\"},\n",
    "            \"chunk_id\": {\"type\":\"text\"},\n",
    "            \"original_index\": {\"type\":\"integer\"},\n",
    "            \"original_content\": {\"type\":\"text\"},\n",
    "            \"contextualized_content\": {\"type\":\"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = open_search_client.indices.create(index_name, body=index_body)\n",
    "\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('bedrock-runtime')\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.0,\n",
    "        }  \n",
    "    ) \n",
    "    response = client.invoke_model(modelId=llm_model_id,body=body)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "# import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "# import anthropic\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class ContextualVectorDB:\n",
    "    def __init__(self, name: str):\n",
    "        self.embedding_client = boto3.client('runtime.sagemaker')\n",
    "        self.llm_client = boto3.client('bedrock-runtime')\n",
    "        self.name = name\n",
    "        self.db_path = f\"./data/{name}/contextual_vector_db.pkl\"\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "\n",
    "        self.token_counts = {\n",
    "            'input': 0,\n",
    "            'output': 0,\n",
    "            'cache_read': 0,\n",
    "            'cache_creation': 0\n",
    "        }\n",
    "        self.token_lock = threading.Lock()\n",
    "\n",
    "    def situate_context(self, doc: str, chunk: str) -> tuple[str, Any]:\n",
    "        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "        CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        body=json.dumps(\n",
    "            {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.0,\n",
    "            }  \n",
    "        ) \n",
    "        response = self.llm_client.invoke_model(modelId=llm_model_id,body=body)\n",
    "        body = json.loads(response.get('body').read())\n",
    "        return body['content'][0]['text'],body['usage']\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
    "\n",
    "        def process_chunk(doc, chunk):\n",
    "            contextualized_text, usage = self.situate_context(doc['content'], chunk['content'])\n",
    "            with self.token_lock:\n",
    "                self.token_counts['input'] += usage['input_tokens']\n",
    "                self.token_counts['output'] += usage['output_tokens']\n",
    "            \n",
    "            return {\n",
    "                'text_to_embed': f\"{chunk['content']}\\n\\n{contextualized_text}\",\n",
    "                'metadata': {\n",
    "                    'doc_id': doc['doc_id'],\n",
    "                    'original_uuid': doc['original_uuid'],\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'original_index': chunk['original_index'],\n",
    "                    'original_content': chunk['content'],\n",
    "                    'contextualized_content': contextualized_text\n",
    "                }\n",
    "            }\n",
    "\n",
    "        print(f\"Processing {total_chunks} chunks with {parallel_threads} threads\")\n",
    "        with ThreadPoolExecutor(max_workers=parallel_threads) as executor:\n",
    "            futures = []\n",
    "            for doc in dataset:\n",
    "                for chunk in doc['chunks']:\n",
    "                    futures.append(executor.submit(process_chunk, doc, chunk))\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=total_chunks, desc=\"Processing chunks\"):\n",
    "                result = future.result()\n",
    "                texts_to_embed.append(result['text_to_embed'])\n",
    "                metadata.append(result['metadata'])\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "\n",
    "        print(f\"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "        print(f\"Total input tokens without caching: {self.token_counts['input']}\")\n",
    "        print(f\"Total output tokens: {self.token_counts['output']}\")\n",
    "    \n",
    "    def _query_endpoint(self,encoded_json, content_type):\n",
    "        response = self.embedding_client.invoke_endpoint(EndpointName=embedder_endpoint_name, ContentType=content_type, Body=encoded_json)\n",
    "        return json.loads(response['Body'].read())\n",
    "\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self._query_endpoint(json.dumps(texts[i : i + batch_size]).encode('utf-8'), 'application/x-text')['embedding']\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        arr = []\n",
    "        for res in result:\n",
    "            arr = arr + res\n",
    "        self.embeddings = arr\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self._query_endpoint(json.dumps([query]).encode('utf-8'), 'application/x-text')['embedding']\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        query_json = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"embedding\": {\n",
    "                        \"vector\": query_embedding[0], \n",
    "                        \"k\": k}\n",
    "                    },\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = open_search_client.search(\n",
    "            body = query_json,\n",
    "            index = index_name\n",
    "        )\n",
    "\n",
    "        top_hits = response['hits']['hits']\n",
    "\n",
    "        results = list(map(lambda x: {'metadata':{'doc_id':x['_source']['doc_id'],'original_uuid':x['_source']['original_uuid'],'chunk_id':x['_source']['chunk_id'],'original_index':x['_source']['original_index'],'original_content':x['_source']['original_content'],'contextualized_content':x['_source']['contextualized_content']},'similarity':x['_score']},top_hits))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_hybrid(self, query: str, k: int = 20,semantic_weight: float = 0.8, bm25_weight: float = 0.2) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self._query_endpoint(json.dumps([query]).encode('utf-8'), 'application/x-text')['embedding']\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        query_json = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [{\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding[0],\n",
    "                                \"k\": k,\n",
    "                                \"boost\": semantic_weight\n",
    "                            }\n",
    "                        }},{\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"original_content\", \"contextualized_content\"],\n",
    "                            \"boost\":bm25_weight\n",
    "                        }}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = open_search_client.search(\n",
    "            body = query_json,\n",
    "            index = index_name\n",
    "        )\n",
    "\n",
    "        top_hits = response['hits']['hits']\n",
    "\n",
    "        results = list(map(lambda x: {'metadata':{'doc_id':x['_source']['doc_id'],'original_uuid':x['_source']['original_uuid'],'chunk_id':x['_source']['chunk_id'],'original_index':x['_source']['original_index'],'original_content':x['_source']['original_content'],'contextualized_content':x['_source']['contextualized_content']},'similarity':x['_score']},top_hits))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load the transformed dataset\n",
    "with open('data/codebase_chunks.json', 'r') as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# Initialize the ContextualVectorDB\n",
    "contextual_db = ContextualVectorDB(\"my_contextual_db\")\n",
    "\n",
    "# Load and process the data\n",
    "contextual_db.load_data(transformed_dataset, parallel_threads=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk index documents into AOSS\n",
    "docs = []\n",
    "for i in range(len(contextual_db.embeddings)):\n",
    "    docs.append({'index':{ \"_index\": index_name}})\n",
    "    docs.append({**contextual_db.metadata[i],'embedding':contextual_db.embeddings[i]})\n",
    "    \n",
    "open_search_client.bulk(body=docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 85.66%\n",
      "Total Score: 0.8565668202764978\n",
      "Total queries: 248\n",
      "Pass@10: 89.78%\n",
      "Total Score: 0.8978494623655914\n",
      "Total queries: 248\n",
      "Pass@20: 93.15%\n",
      "Total Score: 0.9314516129032258\n",
      "Total queries: 248\n"
     ]
    }
   ],
   "source": [
    "r5 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "r10 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "r20 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual BM25\n",
    "\n",
    "Contextual embeddings is an improvement on traditional semantic search RAG, but we can improve performance further. In this section we'll show you how you can use contextual embeddings and *contextual* BM25 together. While you can see performance gains by pairing these techniques together without the context, adding context to these methods reduces the top-20-chunk retrieval failure rate by 42%.\n",
    "\n",
    "BM25 is a probabilistic ranking function that improves upon TF-IDF. It scores documents based on query term frequency, while accounting for document length and term saturation. BM25 is widely used in modern search engines for its effectiveness in ranking relevant documents.\n",
    "\n",
    "One difference between a typical BM25 search and what we'll do in this section is that, for each chunk, we'll run each BM25 search on both the chunk content and the additional context that we generated in the previous section. From there, we'll use a technique called reciprocal rank fusion to merge the results from our BM25 search with our semantic search results. This allows us to perform a hybrid search across both our BM25 corpus and vector DB to return the most optimal documents for a given query.\n",
    "\n",
    "In the function below, we allow you the option to add weightings to the semantic search and BM25 search documents as you merge them with Reciprocal Rank Fusion. By default, we set these to 0.8 for the semantic search results and 0.2 to the BM25 results. We'd encourage you to experiment with different values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class OpenSearchBM25:\n",
    "    def __init__(self):\n",
    "        self.index_name = index_name\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        # self.es_client.indices.refresh(index=self.index_name)  # Force refresh before each search\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"original_content\", \"contextualized_content\"],\n",
    "                }\n",
    "            },\n",
    "            \"size\": k,\n",
    "        }\n",
    "        response = open_search_client.search(index=self.index_name, body=search_body)\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": hit[\"_source\"][\"doc_id\"],\n",
    "                \"original_index\": hit[\"_source\"][\"original_index\"],\n",
    "                \"original_content\": hit[\"_source\"][\"original_content\"],\n",
    "                \"contextualized_content\": hit[\"_source\"][\"contextualized_content\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "            for hit in response[\"hits\"][\"hits\"]\n",
    "        ]\n",
    "\n",
    "def retrieve_advanced(query: str, db: ContextualVectorDB, db_bm25: OpenSearchBM25,k: int, semantic_weight: float = 0.8, bm25_weight: float = 0.2):\n",
    "    num_chunks_to_recall = 150\n",
    "\n",
    "    # Semantic search\n",
    "    semantic_results = db.search(query, k=num_chunks_to_recall)\n",
    "    ranked_chunk_ids = [(result['metadata']['doc_id'], result['metadata']['original_index']) for result in semantic_results]\n",
    "\n",
    "    # BM25 search using OpenSearch\n",
    "    bm25_results = db_bm25.search(query, k=num_chunks_to_recall)\n",
    "    ranked_bm25_chunk_ids = [(result['doc_id'], result['original_index']) for result in bm25_results]\n",
    "\n",
    "    # Combine results\n",
    "    chunk_ids = list(set(ranked_chunk_ids + ranked_bm25_chunk_ids))\n",
    "    chunk_id_to_score = {}\n",
    "\n",
    "    # Initial scoring with weights\n",
    "    for chunk_id in chunk_ids:\n",
    "        score = 0\n",
    "        if chunk_id in ranked_chunk_ids:\n",
    "            index = ranked_chunk_ids.index(chunk_id)\n",
    "            score += semantic_weight * (1 / (index + 1))  # Weighted 1/n scoring for semantic\n",
    "        if chunk_id in ranked_bm25_chunk_ids:\n",
    "            index = ranked_bm25_chunk_ids.index(chunk_id)\n",
    "            score += bm25_weight * (1 / (index + 1))  # Weighted 1/n scoring for BM25\n",
    "        chunk_id_to_score[chunk_id] = score\n",
    "\n",
    "    # Sort chunk IDs by their scores in descending order\n",
    "    sorted_chunk_ids = sorted(\n",
    "        chunk_id_to_score.keys(), key=lambda x: (chunk_id_to_score[x], x[0], x[1]), reverse=True\n",
    "    )\n",
    "\n",
    "    # Assign new scores based on the sorted order\n",
    "    for index, chunk_id in enumerate(sorted_chunk_ids):\n",
    "        chunk_id_to_score[chunk_id] = 1 / (index + 1)\n",
    "\n",
    "    # Prepare the final results\n",
    "    final_results = []\n",
    "    semantic_count = 0\n",
    "    bm25_count = 0\n",
    "    for chunk_id in sorted_chunk_ids[:k]:\n",
    "        chunk_metadata = next(chunk for chunk in db.metadata if chunk['doc_id'] == chunk_id[0] and chunk['original_index'] == chunk_id[1])\n",
    "        is_from_semantic = chunk_id in ranked_chunk_ids\n",
    "        is_from_bm25 = chunk_id in ranked_bm25_chunk_ids\n",
    "        final_results.append({\n",
    "            'chunk': chunk_metadata,\n",
    "            'score': chunk_id_to_score[chunk_id],\n",
    "            'from_semantic': is_from_semantic,\n",
    "            'from_bm25': is_from_bm25\n",
    "        })\n",
    "        \n",
    "        if is_from_semantic and not is_from_bm25:\n",
    "            semantic_count += 1\n",
    "        elif is_from_bm25 and not is_from_semantic:\n",
    "            bm25_count += 1\n",
    "        else:  # it's in both\n",
    "            semantic_count += 0.5\n",
    "            bm25_count += 0.5\n",
    "\n",
    "    return final_results, semantic_count, bm25_count\n",
    "\n",
    "def evaluate_db_advanced(db: ContextualVectorDB, original_jsonl_path: str, k: int):\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    db_bm25 = OpenSearchBM25()\n",
    "    # Warm-up queries\n",
    "    warm_up_queries = original_data[:10]\n",
    "    for query_item in warm_up_queries:\n",
    "        _ = retrieve_advanced(query_item['query'], db, db_bm25, k)\n",
    "    \n",
    "    total_score = 0\n",
    "    total_semantic_count = 0\n",
    "    total_bm25_count = 0\n",
    "    total_results = 0\n",
    "    \n",
    "    for query_item in tqdm(original_data, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if golden_doc:\n",
    "                golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "                if golden_chunk:\n",
    "                    golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs, semantic_count, bm25_count = retrieve_advanced(query, db, db_bm25, k)\n",
    "        \n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['chunk']['original_content'].strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "        \n",
    "        total_semantic_count += semantic_count\n",
    "        total_bm25_count += bm25_count\n",
    "        total_results += len(retrieved_docs)\n",
    "    \n",
    "    total_queries = len(original_data)\n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    \n",
    "    semantic_percentage = (total_semantic_count / total_results) * 100 if total_results > 0 else 0\n",
    "    bm25_percentage = (total_bm25_count / total_results) * 100 if total_results > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "    \n",
    "    print(f\"Pass@{k}: {pass_at_n:.2f}%\")\n",
    "    print(f\"Average Score: {average_score:.2f}\")\n",
    "    print(f\"Total queries: {total_queries}\")\n",
    "    print(f\"Percentage of results from semantic search: {semantic_percentage:.2f}%\")\n",
    "    print(f\"Percentage of results from BM25: {bm25_percentage:.2f}%\")\n",
    "    \n",
    "    return results, {\"semantic\": semantic_percentage, \"bm25\": bm25_percentage}\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "# lexical and semantic search in 1 pass through AOSS query\n",
    "def retrieve_advanced_hybrid(query: str, db: ContextualVectorDB,k: int, semantic_weight: float = 0.8, bm25_weight: float = 0.2):\n",
    "    num_chunks_to_recall = 150\n",
    "\n",
    "    # hybrid search\n",
    "    results = db.search_hybrid(query,k=num_chunks_to_recall,semantic_weight=semantic_weight,bm25_weight=bm25_weight)\n",
    "    ranked_results = [result['metadata']['original_content'] for result in results]\n",
    "\n",
    "    return ranked_results\n",
    "\n",
    "# lexical and semantic search in 1 pass through AOSS query\n",
    "def evaluate_db_hybrid(db: ContextualVectorDB, original_jsonl_path: str, k: int):\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    total_score = 0\n",
    "    total_results = 0\n",
    "    for query_item in tqdm(original_data, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if golden_doc:\n",
    "                golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "                if golden_chunk:\n",
    "                    golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieve_advanced_hybrid(query, db, k)\n",
    "        \n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                if doc.strip() == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "        \n",
    "        total_results += len(retrieved_docs)\n",
    "    \n",
    "    total_queries = len(original_data)\n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    \n",
    "    results = {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "    \n",
    "    print(f\"Pass@{k}: {pass_at_n:.2f}%\")\n",
    "    print(f\"Average Score: {average_score:.2f}\")\n",
    "    print(f\"Total queries: {total_queries}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [04:34<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 86.97%\n",
      "Average Score: 0.87\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 58.43%\n",
      "Percentage of results from BM25: 41.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [04:16<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 89.73%\n",
      "Average Score: 0.90\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 62.80%\n",
      "Percentage of results from BM25: 37.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [04:48<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 93.25%\n",
      "Average Score: 0.93\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 65.94%\n",
      "Percentage of results from BM25: 34.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, combine lexical and semantic search directly through OpenSearch's query dsl using boosting as a substitute for weighted reciprocal rank fusion. Notice the results are much worse than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [02:27<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 62.38%\n",
      "Average Score: 0.62\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [02:34<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 71.59%\n",
      "Average Score: 0.72\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [02:20<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 78.57%\n",
      "Average Score: 0.79\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db_hybrid(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db_hybrid(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db_hybrid(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Re-Ranking Step\n",
    "\n",
    "If you want to improve performance further, we recommend adding a re-ranking step. When using a re-ranker, you can retrieve more documents initially from your vector store, then use your re-ranker to select a subset of these documents. One common technique is to use re-ranking as a way to implement high precision hybrid search. You can use a combination of semantic search and keyword based search in your initial retrieval step (as we have done earlier in this guide), then use a re-ranking step to choose only the k most relevant docs from a combined list of documents returned by your semantic search and keyword search systems.\n",
    "\n",
    "Below, we'll demonstrate only the re-ranking step (skipping the hybrid search technique for now). You'll see that we retrieve 10x the number of documents than the number of final k documents we want to retrieve, then use a re-ranking model from Cohere to select the 10 most relevant results from that list. Adding the re-ranking step delivers a modest additional gain in performance. In our case, Pass@10 improves from 92.81% --> 94.79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, launch the cohere rerank model through [sagemaker jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html#jumpstart-open-use-studio). This requires a marketplace subscription as it is proprietary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_enpoint_name = '<your sagemaker endpoint name for reranker>' # sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohere_aws import Client\n",
    "from typing import List, Dict, Any, Callable\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def chunk_to_content(chunk: Dict[str, Any]) -> str:\n",
    "    original_content = chunk['metadata']['original_content']\n",
    "    contextualized_content = chunk['metadata']['contextualized_content']\n",
    "    return f\"{original_content}\\n\\nContext: {contextualized_content}\" \n",
    "\n",
    "def retrieve_rerank(query: str, db, k: int) -> List[Dict[str, Any]]:\n",
    "    co = Client(endpoint_name=reranker_enpoint_name)\n",
    "    \n",
    "    # Retrieve more results than we normally would\n",
    "    semantic_results = db.search(query, k=k*10)\n",
    "    \n",
    "    # Extract documents for reranking, using the contextualized content\n",
    "    documents = [chunk_to_content(res) for res in semantic_results]\n",
    "\n",
    "    response = co.rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        top_n=k\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    final_results = []\n",
    "    for r in response.results:\n",
    "        original_result = semantic_results[r.index]\n",
    "        final_results.append({\n",
    "            \"chunk\": original_result['metadata'],\n",
    "            \"score\": r.relevance_score\n",
    "        })\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def evaluate_retrieval_rerank(queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20) -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if golden_doc:\n",
    "                golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "                if golden_chunk:\n",
    "                    golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieval_function(query, db, k)\n",
    "        \n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['chunk']['original_content'].strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "    \n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "\n",
    "def evaluate_db_advanced(db, original_jsonl_path, k):\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    \n",
    "    def retrieval_function(query, db, k):\n",
    "        return retrieve_rerank(query, db, k)\n",
    "    \n",
    "    results = evaluate_retrieval_rerank(original_data, retrieval_function, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Average Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [08:22<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 90.24%\n",
      "Average Score: 0.9023617511520737\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [07:17<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 93.08%\n",
      "Average Score: 0.9307795698924731\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [09:03<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 94.66%\n",
      "Average Score: 0.9465725806451613\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
